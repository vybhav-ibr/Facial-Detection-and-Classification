{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGaKilIiTKRs",
        "outputId": "404c2329-7af3-4e48-c0e3-4c79e882f8de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdtJrXEuT1tB",
        "outputId": "95c46e9d-471b-4c31-ec4b-690a02af1ec1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pprint\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "from optparse import OptionParser\n",
        "import pickle\n",
        "import math\n",
        "import cv2\n",
        "import copy\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
        "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n",
        "from keras.utils import get_source_inputs\n",
        "#from keras.utils import layer_utils\n",
        "from tensorflow.python.keras.utils import layer_utils\n",
        "\n",
        "\n",
        "from tensorflow.python.keras.utils import generic_utils\n",
        "from keras.utils import get_file\n",
        "from keras.metrics import categorical_crossentropy\n",
        "\n",
        "from keras.models import Model\n",
        "from tensorflow.python.keras.utils import generic_utils\n",
        "from keras.layers import Layer,InputSpec\n",
        "from keras import initializers, regularizers"
      ],
      "metadata": {
        "id": "0-1SmLf2SMe9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rjCzZhyTR4Tc"
      },
      "outputs": [],
      "source": [
        "def partial_vgg(input_tensor=None):\n",
        "\n",
        "\n",
        "    input_shape = (None, None, 3)\n",
        "\n",
        "    img_input = Input(shape=input_shape)\n",
        "\n",
        "    # Block 1\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "    print(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "    print(x)\n",
        "\n",
        "    # Block 3\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "    print(x)\n",
        "\n",
        "    # Block 4\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "    print(x)\n",
        "\n",
        "    # Block 5\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
        "    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
        "\n",
        "    # We are not using fully connected layers (3 fc layers) as we need feature maps as output from this network.\n",
        "\n",
        "    return x\n",
        "#RPN layer\n",
        "\n",
        "def rpn_layer(base_layers, num_anchors):\n",
        "\n",
        "    #cnn_used for creating feature maps: vgg, num_anchors: 9\n",
        "    x = Conv2D(512, (3, 3), padding='same', activation='relu')(base_layers)\n",
        "\n",
        "    #classification layer: num_anchors (9) channels for 0, 1 sigmoid activation output\n",
        "    x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid')(x)\n",
        "\n",
        "    #regression layer: num_anchors*4 (36) channels for computing the regression of bboxes\n",
        "    x_regr = Conv2D(num_anchors * 4, (1, 1), activation='linear')(x)\n",
        "\n",
        "    return [x_class, x_regr, base_layers] #classification of object(0 or 1),compute bounding boxes, base layers vgg\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RoiPoolingConv(Layer):\n",
        "\n",
        "    def __init__(self, pool_size, num_rois, **kwargs):\n",
        "\n",
        "        self.dim_ordering = K.image_dim_ordering()\n",
        "        self.pool_size = pool_size\n",
        "        self.num_rois = num_rois\n",
        "\n",
        "        super(RoiPoolingConv, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.nb_channels = input_shape[0][3]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "\n",
        "        assert(len(x) == 2)\n",
        "\n",
        "        # x[0] is image with shape (rows, cols, channels)\n",
        "        img = x[0]\n",
        "\n",
        "        # x[1] is roi with shape (num_rois,4) with ordering (x,y,w,h)\n",
        "        rois = x[1]\n",
        "\n",
        "        input_shape = K.shape(img)\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        for roi_idx in range(self.num_rois):\n",
        "\n",
        "            x = rois[0, roi_idx, 0]\n",
        "            y = rois[0, roi_idx, 1]\n",
        "            w = rois[0, roi_idx, 2]\n",
        "            h = rois[0, roi_idx, 3]\n",
        "\n",
        "            x = K.cast(x, 'int32')\n",
        "            y = K.cast(y, 'int32')\n",
        "            w = K.cast(w, 'int32')\n",
        "            h = K.cast(h, 'int32')\n",
        "\n",
        "            # Resized roi of the image to pooling size (7x7)\n",
        "            rs = tf.image.resize_images(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))\n",
        "            outputs.append(rs)\n",
        "\n",
        "\n",
        "        final_output = K.concatenate(outputs, axis=0)\n",
        "                # Reshape to (1, num_rois, pool_size, pool_size, nb_channels) : (1, 4, 7, 7, 3)\n",
        "        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n",
        "\n",
        "        # permute_dimensions is similar to transpose\n",
        "        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n",
        "\n",
        "        return final_output\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'pool_size': self.pool_size,\n",
        "                  'num_rois': self.num_rois}\n",
        "        base_config = super(RoiPoolingConv, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "metadata": {
        "id": "qaGFa17MSKw4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classifier_layer(base_layers, input_rois, num_rois, nb_classes = 4):\n",
        "\n",
        "    # base_layers: vgg\n",
        "    #input_rois: `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n",
        "    #num_rois: number of rois to be processed in one time (4 in here)\n",
        "\n",
        "    input_shape = (num_rois,7,7,512)\n",
        "\n",
        "    pooling_regions = 7\n",
        "\n",
        "    # out_roi_pool.shape = (1, num_rois, channels, pool_size, pool_size)\n",
        "    # num_rois (4) 7x7 roi pooling\n",
        "    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\n",
        "\n",
        "    # Flatten the convlutional layer and connected to 2 FC and 2 dropout\n",
        "    out = Flatten(name='flatten')(out_roi_pool) #expanded into a vector with 25,088 (7×7×512) channels.\n",
        "    out = Dense(4096, activation='relu', name='fc1')(out)\n",
        "    out = Dropout(0.5)(out)\n",
        "    out = Dense(4096, activation='relu', name='fc2')(out)\n",
        "    out = Dropout(0.5)(out)\n",
        "\n",
        "    # two output layer- classifier and regressor\n",
        "    # for classify the class name of the object\n",
        "    out_class = Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name=='dense_class_{}'.format(nb_classes)(out)\n",
        "\n",
        "    #for bboxes coordinates regression\n",
        "    out_regr = Dense(4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name=='dense_regress_{}'.format(nb_classes)(out)\n",
        "\n",
        "    return [out_class, out_regr]"
      ],
      "metadata": {
        "id": "gyoO9D0BW58i"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fffffffffffffffffffffffffffffffffffffffffff\n"
      ],
      "metadata": {
        "id": "QW27vJ52APqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import gzip"
      ],
      "metadata": {
        "id": "Gl-nGaTLARn2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image, ImageDraw\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "from torchvision.models.detection. faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "import gzip\n",
        "import sys\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpj3anMEAWXl",
        "outputId": "3a8ca9d5-1e2a-41f8-c1b0-5c20222d6997"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile(r'drive/MyDrive/face_img.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data_folder')\n",
        "train =pd.read_csv(r\"data_folder/faces.csv\")\n",
        "train=train[['image_name','x0','y0','x1','y1']]"
      ],
      "metadata": {
        "id": "irP54B3VA6-5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()\n",
        "train=train.head(100)"
      ],
      "metadata": {
        "id": "eXqZuGW-BYB-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import zip\n"
      ],
      "metadata": {
        "id": "qLD9rCYtLRjA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_imgs=train.image_name.unique()\n",
        "len(unique_imgs)\n",
        "\n",
        "#12.29\n",
        "unique_imgs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVZm2cFdHQyz",
        "outputId": "c3db6db7-6cf8-41e1-9cf3-a926dcb0ce0e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['00001722.jpg', '00001044.jpg', '00001050.jpg', '00001736.jpg',\n",
              "       '00003121.jpg', '00000400.jpg', '00002571.jpg', '00000366.jpg',\n",
              "       '00002565.jpg', '00001939.jpg', '00001087.jpg', '00001093.jpg',\n",
              "       '00001905.jpg', '00001911.jpg', '00002997.jpg', '00003451.jpg',\n",
              "       '00001246.jpg', '00001520.jpg', '00002029.jpg', '00003337.jpg',\n",
              "       '00003323.jpg', '00001252.jpg', '00002983.jpg', '00003445.jpg',\n",
              "       '00002015.jpg', '00000602.jpg', '00000616.jpg', '00002001.jpg',\n",
              "       '00002767.jpg', '00000170.jpg', '00002954.jpg', '00003492.jpg',\n",
              "       '00000825.jpg', '00000831.jpg', '00002798.jpg', '00002940.jpg',\n",
              "       '00003486.jpg', '00000819.jpg', '00001332.jpg', '00000992.jpg',\n",
              "       '00003243.jpg', '00000986.jpg', '00001440.jpg', '00001326.jpg',\n",
              "       '00002607.jpg', '00002161.jpg', '00001468.jpg', '00000762.jpg',\n",
              "       '00002613.jpg', '00000004.jpg', '00000789.jpg', '00000951.jpg',\n",
              "       '00001497.jpg', '00001483.jpg', '00002808.jpg', '00003041.jpg',\n",
              "       '00000548.jpg'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train=train.drop_duplicates(subset='image_name')\n",
        "len(train)\n",
        "'''\n",
        "for row in train.iterrows():\n",
        "  if row['image_name'] not in unique_imgs:\n",
        "    print(row['image_name'])\n",
        "    '''\n",
        "len(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daXnFvNyMXgb",
        "outputId": "67f1e402-bbe5-4a16-e888-0ffa940ec0f8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustDat(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, df, unique_imgs, indices):\n",
        "    self.df=df\n",
        "    self.unique_imgs=unique_imgs\n",
        "    self.indices=indices\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.indices)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image_name = self.unique_imgs[self.indices[idx]]\n",
        "\n",
        "    boxes=self.df[self.df.image_name == image_name].values[:, 1:].astype(\"float\")\n",
        "    img=Image.open(\"data_folder/images/\" + image_name).convert('RGB')\n",
        "    labels = torch.ones((boxes.shape[0]), dtype = torch.int64)\n",
        "    target = {}\n",
        "    target[\"boxes\"] =torch.tensor (boxes)\n",
        "    target[\"label\"] = labels\n",
        "    return T. ToTensor() (img), target"
      ],
      "metadata": {
        "id": "Ow5mq1nSCV6V"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inds,val_inds=train_test_split(range(train.shape[0]),test_size=0.1)\n",
        "def custom_collate(data):\n",
        "  return data"
      ],
      "metadata": {
        "id": "djHlVlrtECrJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = torch.utils.data.DataLoader(CustDat(train,train.image_name.unique(),train_inds),\n",
        "                                batch_size=10,\n",
        "                                shuffle=True,\n",
        "                                collate_fn=custom_collate,\n",
        "                                pin_memory= True if torch.cuda.is_available() else False)\n",
        "\n",
        "val_dl = torch.utils.data.DataLoader(CustDat(train,train.image_name.unique(),val_inds),\n",
        "                                batch_size=8,\n",
        "                                shuffle=True,\n",
        "                                collate_fn=custom_collate,\n",
        "                                pin_memory= True if torch.cuda.is_available() else False)\n"
      ],
      "metadata": {
        "id": "wIFfckDaE8Ds"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained =True)\n",
        "num_classes=3\n",
        "in_features=model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor=FastRCNNPredictor(in_features,num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N6hGk2bH8Ex",
        "outputId": "70ceafea-b816-474c-8feb-4e5df67a18df"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ead1EN2KIvuo",
        "outputId": "f71d42dc-c6f7-493f-895e-47c0009c53e2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer=torch.optim.SGD(model.parameters(),lr=0.001,momentum=0.9,weight_decay=0.0005)\n",
        "num_epochs=2"
      ],
      "metadata": {
        "id": "6ADXnM13JJQ6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "for epochs in range(num_epochs):\n",
        "  epoch_loss=0\n",
        "  for data in train_dl:\n",
        "    imgs=[]\n",
        "    targets=[]\n",
        "    for d in data:\n",
        "      imgs.append(d[0].to(device))\n",
        "      targ={}\n",
        "      targ['boxes']=d[1][\"boxes\"].to(device)\n",
        "      targ[\"labels\"]=d[1][\"label\"].to(device)\n",
        "      targets.append(targ)\n",
        "    loss_dict=model(imgs,targets)\n",
        "    loss=sum(v for v in loss_dict.values())\n",
        "    epoch_loss+=loss.cpu().detach().numpy()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(epoch_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "ru6_s_0mJebO",
        "outputId": "3a4d9d62-26e4-4642-82a2-500f69c91064"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-07136b5b650a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mepoch_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 904.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 368.81 MiB is free. Process 68664 has 14.38 GiB memory in use. Of the allocated memory 11.93 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "data=iter(val_dl).__next__()"
      ],
      "metadata": {
        "id": "wO4uSLzH68Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img=data[0][0]\n",
        "boxes=data[0][1][\"boxes\"]\n",
        "labels=data[0][1][\"label\"]"
      ],
      "metadata": {
        "id": "1kyeIEKz7G9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output=model([img.to(device)])"
      ],
      "metadata": {
        "id": "aWlACh207VsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_bbox=output[0][\"boxes\"]\n",
        "out_scores=output[0][\"scores\"]"
      ],
      "metadata": {
        "id": "HvdUL5-N7m3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keep=torchvision.ops.nms(out_bbox,out_scores,.45)\n",
        "out_bbox.shape,keep.shape"
      ],
      "metadata": {
        "id": "Sg49OYhO71Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im = (img.permute(1,2,0).cpu().detach().numpy()=255).astype('uint8')"
      ],
      "metadata": {
        "id": "JwMxbmaR8Jql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vsample=Image.fromarray(im)\n",
        "draw=ImageDraw.draw(vsample)\n",
        "for box in boxes:\n",
        "  draw.rectangle(list(box),fill=None,outline='red')\n",
        "vsample"
      ],
      "metadata": {
        "id": "43b53G858qd8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}